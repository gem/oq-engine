#!/usr/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

"""Deterministic Risk Computations based on Hazard, Exposure and Vulnerability

Expects to receive:
    Shakemap (ground motion per grid cell)
    Exposure (value per grid cell)
    Vulnerability functions (multiple lists per grid cell)
    Region of interest

It can receive these either through gflags (current functionality), or
through a configuration file.

Expects to compute:
    A grid of loss-ratio curves and store the results in XML
    A grid of loss curves and store the results in XML 
    A map of losses at each interval and store the results in GeoTIFF

"""

import os
import sys

# this is a hack so that it is easier to test these scripts,
# it will add the proper directories to the path so that 
# this script can be run from a checkout
if os.path.exists(os.path.join(os.path.dirname(os.path.dirname(__file__)),
                  'opengem')):
    sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from fabric.api import env, local, run
from multiprocessing import Process

from opengem import flags
from opengem import jobber
from opengem import logs

from opengem import config
from opengem.output import geotiff
from opengem.parser import exposure
from opengem.parser import vulnerability


FLAGS = flags.FLAGS

flags.DEFINE_string('risk_config', 'risk-config.gem', 'Risk configuration file')
flags.DEFINE_string('hazard_config', 'hazard-config.gem', 'Hazard configuration file')

flags.DEFINE_boolean('profile', False, 'Run profiler?')
flags.DEFINE_boolean('load_profile', False, 'Load profiler data?')
flags.DEFINE_string('profile_log', 'gem-risk.profile', 'Profiling log')

flags.DEFINE_boolean('partition', False, 'Partition job?')

flags.DEFINE_boolean('server', False, 'Launch memcached and RabbitMQ subprocs')

flags.DEFINE_boolean('worker', False, 'Launch celery subprocs')

def _launch_worker_subprocs():
    """Start celery as python subprocs.

    There should only be one copy of celery running.

    If a process is already running, don't kill it
    and carry on.

    If a process is not running, launch it as a
    multiprocessing.Process
    """
    # set the host string (otherwise, the user will be prompted for one)
    env.host_string = 'localhost'
    
    # launch celery if it's not already running
    if not _is_running('[c]elery'):
        print "Starting celery..."
        Process(target=local, args=('celeryd',)).start()

def _launch_server_subprocs():
    """Start memcached and RabbitMQ as python subprocs.

    There should only be one copy of RabbitMQ running  
    and (for now) only one copy of memcached.

    If a process is already running, don't kill it
    and carry on.

    If a process is not running, launch it as a
    multiprocessing.Process
    """
    # set the host string (otherwise, the user will be prompted for one)
    env.host_string = 'localhost'

    
    # launch rabbitmq if it's not already running
    if not _is_running('[r]abbitmq'):
        print "Starting rabbitmq..."
        Process(target=local, args=('sudo /usr/local/sbin/rabbitmq-server',)).start()

 
    # launch memcached if it's not already running
    if not _is_running('[m]emcached'):
        print "Starting memcached..."
        Process(target=local, args=('memcached',)).start()

def _is_running(proc_name):
    """Checks to see if a process is already
    running by executing a 'ps aux | grep proc_name'.

    Be sure to surroud the first letter
    of the proc_name with [] to avoid getting
    a false positive from the grep
    (example: '[m]emcached', instead of 'memcached').
    """
    # set fabric to warn only (so fabric run() calls with no
    # response don't crash and burn)
    env.warn_only = True

    is_running = run('ps aux | grep %s' % proc_name) != ''
    # reset warn only
    env.warn_only = False

    return is_running

if __name__ == '__main__':
    args = FLAGS(sys.argv)
    logs.init_logs()
    
    # Collect inputs
    # Determine Processing type
    # Validate input data
    
    # Prepare final configuration, save it
    # Hash final config, store that
    
    # Kick off processing tasks, and wait...
    # Collate results
    # Generate output
    
    if FLAGS.profile:
        import cProfile
        cProfile.run('tasks.main(FLAGS.vulnerability, \
                        FLAGS.hazard_curves, FLAGS.region, \
                        FLAGS.exposure, FLAGS.loss_map)', FLAGS.profile_log)
    elif FLAGS.load_profile:
        import pstats
        p = pstats.Stats(FLAGS.profile_log)
        p.sort_stats('cumulative').print_stats(30)    
    elif FLAGS.server:
        # launch memcached and rabbitmq
        _launch_server_subprocs()
    elif FLAGS.worker:
        # launch celery
        _launch_worker_subprocs()
    else:
        job = config.Job.from_files(FLAGS.risk_config, FLAGS.hazard_config)
        job.to_kvs()
        
        if not jobber.Jobber(job, FLAGS.partition).run():
            # TODO (ac): Should we print additional details?
            logs.LOG.critical("The job configuration is inconsistent, "
                    "aborting computation.")
