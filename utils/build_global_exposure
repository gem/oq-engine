#!/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
# Copyright (C) 2023, GEM Foundation
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import logging
import operator
import pandas
import numpy
import h5py
from openquake.baselib import hdf5, sap, performance, general
from openquake.baselib.parallel import Starmap
from openquake.hazardlib import nrml
from openquake.hazardlib.geo.utils import geohash3
from openquake.commonlib.datastore import build_dstore_log
from openquake.commonlib.oqvalidation import OqParam
from openquake.risklib.riskmodels import CompositeRiskModel, RiskFuncList
from openquake.risklib.asset import _get_exposure
from openquake.qa_tests_data import mosaic

U16 = numpy.uint16
U32 = numpy.uint32
F32 = numpy.float32
CONV = {n: F32 for n in '''
BUILDINGS COST_CONTENTS_USD COST_NONSTRUCTURAL_USD
COST_STRUCTURAL_USD LATITUDE LONGITUDE OCCUPANTS_PER_ASSET
OCCUPANTS_PER_ASSET_AVERAGE OCCUPANTS_PER_ASSET_DAY
OCCUPANTS_PER_ASSET_NIGHT OCCUPANTS_PER_ASSET_TRANSIT
TOTAL_AREA_SQM'''.split()}
CONV['ASSET_ID'] = (numpy.bytes_, 24)
for f in (None, 'ID_1'):
    CONV[f] = str
TAGS = {'TAXONOMY': [], 'ID_0': [], 'ID_1': [], 'OCCUPANCY': []}
IGNORE = set('NAME_0 NAME_1 SETTLEMENT TOTAL_REPL_COST_USD COST_PER_AREA_USD'
             .split())
MOSAIC_DIR = os.path.dirname(mosaic.__file__)
FIELDS = {'TAXONOMY', 'COST_NONSTRUCTURAL_USD', 'LONGITUDE',
          'COST_CONTENTS_USD', 'ASSET_ID', 'OCCUPANCY',
          'OCCUPANTS_PER_ASSET', 'OCCUPANTS_PER_ASSET_AVERAGE',
          'OCCUPANTS_PER_ASSET_DAY', 'OCCUPANTS_PER_ASSET_NIGHT',
          'OCCUPANTS_PER_ASSET_TRANSIT', 'TOTAL_AREA_SQM',
          'BUILDINGS', 'COST_STRUCTURAL_USD',
          'LATITUDE', 'ID_0', 'ID_1'}


def add_geohash3(array):
    """
    Add field "geohash3" to a structured array
    """
    if len(array) == 0:
        return ()
    dt = array.dtype
    dtlist = [('geohash3', U16)] + [(n, dt[n]) for n in dt.names]
    out = numpy.zeros(len(array), dtlist)
    for n in dt.names:
        out[n] = array[n]
        out['geohash3'] = geohash3(array['LONGITUDE'], array['LATITUDE'])
    return out


def fix(arr):
    # prepend the country to ASSET_ID and ID_1
    ID0 = arr['ID_0']
    ID1 = arr['ID_1']
    arr['ASSET_ID'] = numpy.char.add(numpy.array(ID0, 'S3'), arr['ASSET_ID'])
    for i, (id0, id1) in enumerate(zip(ID0, ID1)):
        if not id1.startswith(id0):
            ID1[i] = '%s-%s' % (id0, ID1[i])


def collect_exposures(grm_dir):
    """
    Collect the files of kind Exposure_<Country>.xml.

    :returns: exposure, xmlfiles, csvfiles
    """
    out = []
    csvfiles = []
    for region in os.listdir(grm_dir):
        expodir = os.path.join(grm_dir, region, 'Exposure', 'Exposure')
        if not os.path.exists(expodir):
            continue
        for fname in os.listdir(expodir):
            if fname.startswith('Exposure_'):  # i.e. Exposure_Chile.xml
                fullname = os.path.join(expodir, fname)
                out.append(fullname)
                exposure, _ = _get_exposure(fullname)
                csvfiles.extend(exposure.datafiles)
    return exposure, out, csvfiles


def exposure_by_geohash(array, monitor):
    """
    Yields pairs (geohash, array)
    """
    array = add_geohash3(array)
    fix(array)
    for gh in numpy.unique(array['geohash3']):
        yield gh, array[array['geohash3']==gh]


def store_tagcol(dstore):
    """
    A TagCollection is stored as arrays like taxonomy = [
    "?", "Adobe", "Concrete", "Stone-Masonry", "Unreinforced-Brick-Masonry",
    "Wood"] with attributes __pyclass__, tagnames, tagsize
    """
    tagsizes = []
    tagnames = []
    for tagname in TAGS:
        name = 'taxonomy' if tagname == 'TAXONOMY' else tagname
        tagnames.append(name)
        tagvalues = numpy.concatenate(TAGS[tagname])
        uvals, inv, counts = numpy.unique(tagvalues, return_inverse=1, return_counts=1)
        size = len(uvals) + 1
        tagsizes.append(size)
        logging.info('Storing %s[%d/%d]', tagname, size, len(inv))
        hdf5.extend(dstore[f'assets/{tagname}'], inv + 1)  # indices start from 1
        dstore['tagcol/' + name] = numpy.concatenate([['?'], uvals])
        if name == 'ID_0':
            dtlist = [('country', (numpy.bytes_, 3)), ('counts', int)]
            arr = numpy.empty(len(uvals), dtlist)
            arr['country'] = uvals
            arr['counts'] = counts
            dstore['assets_by_country'] = arr

    dic = dict(__pyclass__='openquake.risklib.asset.TagCollection',
               tagnames=numpy.array(tagnames, hdf5.vstr),
               tagsizes=tagsizes)
    dstore.getitem('tagcol').attrs.update(dic)


def gen_tasks(files, monitor):
    """
    Generate tasks of kind exposure_by_geohash for large files
    """
    for file in files:
        # read CSV in chunks
        dfs = pandas.read_csv(
            file.fname, names=file.header, dtype=CONV,
            usecols=file.fields, skiprows=1, chunksize=1_000_000)
        for i, df in enumerate(dfs):
            if len(df) == 0:
                continue
            if 'ID_1' not in df.columns:  # happens for many islands
                df['ID_1'] = '???'
            dt = hdf5.build_dt(CONV, df.columns, file.fname)
            array = numpy.zeros(len(df), dt)
            for col in df.columns:
                array[col] = df[col].to_numpy()
            if i == 0:
                yield from exposure_by_geohash(array, monitor)
            else:
                print(file.fname)
                yield exposure_by_geohash, array


def read_world_exposure(grm_dir, dstore):
    """
    Read the exposure files for the entire world (assume some conventions
    on the file names are respected).

    :param grm_dir: directory containing the global risk model
    """
    exposure, fnames, csvfiles = collect_exposures(grm_dir)
    dstore['exposure'] = exposure
    files = hdf5.sniff(csvfiles, ',', IGNORE)
    dtlist = [(t, U32) for t in TAGS] + \
        [(f, F32) for f in set(CONV)-set(TAGS)-{'ASSET_ID', None}] + \
        [('ASSET_ID', h5py.string_dtype('ascii', 25))]
    for name, dt in dtlist:
        logging.info('Creating assets/%s', name)
    dstore.create_df('assets', dtlist, 'gzip')
    slc_dt = numpy.dtype([('gh3', U16), ('start', U32), ('stop', U32)])
    dstore.create_dset('assets/slice_by_gh3', slc_dt, fillvalue=None)
    dstore.swmr_on()
    smap = Starmap.apply(gen_tasks, (files,),
                         weight=operator.attrgetter('size'), h5=dstore.hdf5)
    num_assets = 0
    # NB: we need to keep everything in memory to make gzip efficient
    acc = general.AccumDict(accum=[])
    for gh3, arr in smap:
        for name in FIELDS:
            if name in TAGS:
                TAGS[name].append(arr[name])
            else:
                acc[name].append(arr[name])
        n = len(arr)
        slc = numpy.array([(gh3, num_assets, num_assets + n)], slc_dt)
        hdf5.extend(dstore['assets/slice_by_gh3'], slc)
        num_assets += n
    Starmap.shutdown()
    for name in sorted(acc):
        lst = acc.pop(name)
        arr = numpy.concatenate(lst, dtype=lst[0].dtype)
        logging.info(f'Storing assets/{name}')
        hdf5.extend(dstore['assets/' + name], arr)
    store_tagcol(dstore)

    # sanity check
    for name in FIELDS:
        n = len(dstore['assets/' + name])
        assert n == num_assets, (name, n, num_assets)

    return num_assets


def read_world_vulnerability(grm_dir, dstore):
    """
    Store the world CompositeRiskModel
    """
    kinds = ['structural', 'nonstructural', 'contents', 'area', 'number',
             'fatalities', 'residents']
    vfuncs = RiskFuncList()
    for kind in kinds:
        name = f'Vulnerability/vulnerability/vulnerability_{kind}.xml'
        fname = os.path.join(grm_dir, name)
        for vf in nrml.to_python(fname).values():
            vf.loss_type = 'occupants' if kind == 'fatalities' else kind
            vf.kind = 'vulnerability'
            vfuncs.append(vf)
    oq = OqParam(calculation_mode='custom')
    crmodel = CompositeRiskModel(oq, vfuncs)
    dstore.create_df('crm', crmodel.to_dframe(),
                     'gzip', **crmodel.get_attrs())
    return len(vfuncs)


def read_world_tmap(grm_dir, dstore):
    """
    Store the world taxonomy mapping
    """
    # get the names of the files to read
    tmap_df = pandas.read_csv(os.path.join(MOSAIC_DIR, 'taxonomy_mapping.csv'),
                              index_col=['country'])
    dic = {}
    for fname, df in tmap_df.groupby('fname'):
        dic[fname] = '_'.join(sorted(df.index))
    n = len(dic)
    assert len(set(dic.values())) == n, sorted(dic.values())
    for cwd, dirs, files in os.walk(grm_dir):
        for f in files:
            if f in dic:
                df = pandas.read_csv(os.path.join(cwd, f))
                try:
                    dstore.create_df('tmap/' + dic[f], df)
                except ValueError:  # exists already
                    print('Repeated %s' % dic[f])
    return n


def main(grm_dir):
    """
    Storing global exposure
    """
    mon = performance.Monitor(measuremem=True)
    dstore, log = build_dstore_log()
    with dstore, log:
        with mon:
            n = read_world_vulnerability(grm_dir, dstore)
            logging.info('Read %d vulnerability functions', n)
            n = read_world_tmap(grm_dir, dstore)
            logging.info('Read %d taxonomy mappings', n)
            n = read_world_exposure(grm_dir, dstore)
            logging.info('Created {} with {:_d} assets'.
                         format(dstore.filename, n))
        logging.info(mon)


main.grm_dir = 'global risk model directory'


if __name__ == '__main__':
    sap.run(main)
