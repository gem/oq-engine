Event based/scenario risk calculations in 2021
==============================================

Between engine 3.11 (released in March 2021) and engine 3.12 (released
in XXX 2021) the event based and scenario risk calculators have been
completely rewritten.

The rewrite was unavoidable: on one side it was required to fix some
design shortcomings of the current calculators, on the other is was
needed to accomodate for new strategies, in particular to manage the
uncertainty on the vulnerability functions via an additional
Montecarlo sampling.

Shortcomings of the calculators in 2020 and before
--------------------------------------------------

The `event_based_risk` and `scenario_risk` calculators had serious limitations.

1. In presence of nonzero coefficients of variation in vulnerability
functions with a lognormal ("LN") distribution, the old calculators
required building and storing a huge matrix (called the epsilon matrix)
of size A x E begin A the number of assets and E the number of events.
For essentially all risk calculations except toy models building the
epsilon matrix was impossible (for instance for 1 million assets and 1
million events, which is a small calculation, 7+ terabytes are required!).
Even in the case the matrix was small enough that could be
stored (for instance for 100,000 assets and 100,000 events "only" 74 GB
are needed) the performance of the calculation was impossibly poor,
meaning that it was impossible running the calculation since the workers
were blocked forever reading data concurrently from the master node.
To work around this issue the engine had an option in the job.ini file,
*ignore_covs=true*, that allowed to run the calculation by setting all
the coefficients of variation to zero, therefore ignoring the uncertainty
on the vulnerability functions and not building the epsilon matrix.

2. Even using *ignore_covs=true* the `event_based_risk` calculator could not
perform very large calculations, the bottleneck begin the reading of the
ground motion fields. If too many GMFs were generated, then it was impossible
to read them since engine 3.11 required to keep them all in memory in the
master node and to perform a group by site operation using a huge amount
of memory, like hundreds of gigabytes for a few dozens of gigabytes of GMFs.
Engine 3.10 and older used a different strategy, not requiring the group by,
but still large calculations were impossible, with the workers being stuck
reading concurrently the GMFs.
The work around this issue the `ebrisk` calculator had been introduced in
2018, which was able to generate the GMFs on-the-fly, thus skipping the
reading step, at the cost of not being able to store the GMFs. Therefore
it was impossible to reuse the same hazard for performing different risk
calculations. The `ebrisk` calculator had other limitations too, like
not being able to compute the average losses for each realization, nor
the quantiles, but only the mean average losses.

3. In presence of vulnerability functions with a beta distribution ("BT"),
the `ebrisk` calculator was producing different numbers depending of the
number of spawned tasks. While the results were statistically correct,
it is very annoying not to be able to produce exactly the same numbers
independently from the number of tasks. It means that a calculation on a
laptop (few cores => few tasks) was not giving results identical
to a computation on a cluster (many cores => many tasks).

4. It is very annoying to have two calculators instead of one, since
it is very easy for them to get misaligned and to start producing
different numbers. The situation was particularly bad since we had 3
different calculators (`scenario_risk`, `event_based_risk`, `ebrisk`)
where there should have been a single calculator. In particular,
issues 1 and 3 were indirectly caused by the non-unification of the
calculators. In order to ensure task-independency with the LN
distribution both with the `ebrisk` and the `event_based_risk`
calculator it was necessary to store the matrix of the epsilons; on
the contrary, in case of the beta distribution we were not storing
anything and therefore the produced losses were different between
`event_based_risk` and `ebrisk`, and even task-dependent for `ebrisk`.

Now all issues 1, 2, 3 and 4 are fully solved. Engine 3.11 merged together
`scenario_risk` and `event_based_risk`, while engine 3.12 merged even
`ebrisk`. Now all calculators use the `ebrisk` strategy (distribution by
event, not  by site) and the random numbers are produced consistently.

Best practices
-----------------------------

In the past the `ebrisk` calculator was a lot better than the `event_based_risk`
calculator and it was the recommended way to run large calculations. Now
they are the same. What you can do is to work on the *ground_motion_fields*
flag. With

`ground_motion_fields = true`

the GMFs will be stored on the file system and will be reused in risk
calculations, if wanted. The reading part will be extremely efficient
even with 100+ GB of GMFs, your limit is the capacity of your disk, really.
With

`ground_motion_fields = true`

it will not be possible to reuse the hazard, but you will save disk space
and you will be a bit faster.

Open issues
-------------------

Right now the damage calculator is distributing by site and not
by event like the risk calculator. This is an issue, since it
kills the performance of large calculations and makes it impossible to
use the tricks used in risk. Switching to the distribution by
event is possible but tricky since keeping the random number strategy as it
is will cause a dependency on the number of concurrent tasks. Therefore we
will have to change how the seeds are set in the function `bin_ddd`.
