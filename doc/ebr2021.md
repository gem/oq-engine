Event based/scenario risk calculations in 2021
==============================================

Between engine 3.11 (released in March 2021) and engine 3.12 (released
in XXX 2021) the event based and scenario risk calculators have been
completely rewritten.

The rewrite was unavoidable: on one side it was needed to fix some
design shortcomings, while on the other side is was needed to
accomodate for new strategies in the risk calculators, in particular
to manage the uncertainty on the vulnerability functions via an
additional Montecarlo sampling.

The rewriting changed the performance characteristic of the calculators
significantly and therefore a new document describing the "best practices"
to run large calculations is needed. Here we will do that, as well as
motivating the reasons for the change.

Shortcomings of the calculators in 2020 and before
--------------------------------------------------

The event_based_risk and scenario_risk calculators had serious limitations.

1. In presence of nonzero coefficients of variation in vulnerability
functions with a lognormal ("LN") distribution, the old calculators
required building and storing a huge matrix (called the epsilon matrix)
of size A x E begin A the number of assets and E the number of events.
For essentially all risk calculations except toy models building the
epsilon matrix was impossible (for instance for 1 million assets and 1
million events, which is a small calculation, the matrix requires 7+
terabytes!). Even in the case the matrix was small enough that could be
stored (for instance for 100,000 assets and 100,000 events "only" 74 GB
are needed) the performance of the calculation was impossibly poor,
meaning that it was impossible running the calculation since the workers
were blocked forever reading data concurrently from the master node.
To work around this issue the engine had an option of the job.ini,
*ignore_covs=true*, that allowed to run the calculation by setting all
the coefficients of variation to zero, therefore ignoring the uncertainty
on the vulnerability functions and not building the epsilon matrix.

2. Even using *ignore_covs=true* the event_based_risk calculator could not
perform very large calculations, the bottleneck begin the reading of the
ground motion fields. If too many GMFs are generated, then it was impossible
to read them since engine 3.11 required to keep them all in memory in the
master node and to perform a group by site ID operation, that uses a lot
of memory, like hundreds of gigabytes for a few dozens of gigabytes of GMFs.
Engine 3.10 and older used a different strategy, not requiring the group by,
but still large calculations were impossible, with the workers being stuck
reading concurrently the GMFs.
The work around this issue the "ebrisk" calculator has been introduced in
2018, which was able to generate the GMFs on-the-fly, thus skipping the
reading step, at the cost of not being able to store the GMFs. Therefore
it was impossible to reuse the same hazard for performing different risk
calculations. The "ebrisk" calculator had other limitations too, like
not being able to compute the average losses for each realization, nor
the quantiles, but only the mean average losses.

3. In presence of vulnerability functions with a beta distribution ("BT"),
the ebrisk calculator was producing different numbers depending of the
number of spawned tasks. While the results were statistically correct,
it is very annoying not to be able to produce exactly the same numbers
independently from the number of tasks. It means that a calculation on a
laptop (few cores => few tasks) was not giving results identical
to a computation on a cluster (many cores => many tasks).

4. It is very annoying to have two calculators instead of one, since it
is very easy for them to get misaligned and to start producing different
numbers. The situation was particularly bad since we had 3 different
calculators (scenario_risk, event_based_risk, ebrisk) where there should
have been a single calculator. In particular, issues 1 and 3 were indirectly
caused by the non-unification of the calculators. In order to ensure
task-independency with the LN distribution both with the ebrisk and the
event_based_risk calculator it was necessary to store the matrix of the
epsilons; in case of the beta distribution we were not storing anything
and therefore the produced losses were different between event_based_risk
and ebrisk, and even task-dependent for ebrisk.

Now all issues 1, 2, 3 and 4 are fully solved. Engine 3.11 merged together
scenario_risk and event_based_risk, while engine 3.12 merged together even
ebrisk. Now all calculators use the ebrisk strategy and the random numbers
are produced consistently. Before this was impossible because the
scenario_risk/event_based_risk were using a distribution by site strategy
while the ebrisk was using a distribution by rupture strategey.

Best practices
-----------------------------

In the past the ebrisk calculator was a lot better than the event_based_risk
calculator and it was the recommended way to run large calculations. Now
they are the same. What you can do is to work on the *ground_motion_fields*
flag. With

`ground_motion_fields = true`

the GMFs will be stored on the file system and will be reused in risk
calculations, if wanted. The reading part will be extremely efficient
even with 100+ GB of GMFs, your limit is the capacity of your disk, really.
With

`ground_motion_fields = true`

it will not be possible to reuse the hazard, but you will save disk space
and you will be a bit faster.


Open issues
-------------------

Right now the damage calculator is distributing by site ID and not
by event like the risk calculator. This is an issue, since it
kills the performance of large calculations and makes it impossible to
use the tricks used in risk. Switching to the distribution by
event is possible but tricky since keeping the random number strategy as it
is will cause a dependency on the number of concurrent tasks. Therefore we
will have to change how the seeds are set in the function `bin_ddd`.
