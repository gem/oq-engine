* Single site optimization

Between the years 2021 and 2022 a large project was undertaken to
speedup single site PSHA calculations, the so-called *vectorization*
project. The project had with positive effects on all calculations,
even the ones with multiple sites, but the less sites are involved
the more the vectorization helps. The key idea was to supplement
vectorization by site (which was already there, but completely ineffective
and actually counterproductive when there is a single site) with
vectorization by rupture. Technically that involved converting the
context objects (capturing rupture parameters, site parameters
and rupture-site distances) into numpy arrays and to change the
GMPE API to consume such arrays. Then two classes of speedups
were made possible:

1. speedup of the GMPE computations due to the usage of numpy arrays
   in =computing mean_std=
2. speedup of the context generation due to the usage numba in =make_contexts=
   (implemented only for point-like sources)

For non-experts, numpy is a C-extension making array calculations
going at C-speed for Python and numba is a LLVM-based compiler which
in the best case is able to reach C-speeds for non-vectorized code
(see [[https://numba.pydata.org/]]).

Here we will quantify the improvements in a few selected calculations
by comparing engine version 3.11 (the version released just before the
vectorization project started) and engine version 3.15 (released at
the end of the vectorization project).

** EDF disaggregation

Our user Guillaume Daniel sent us a disaggregation calculation that
was particularly slow, taking more than 12 hours of cumulative time on
a single core machine. The model contained only area sources and
most of the time was spent building planar surfaces and then
computing distances in =make_contexts=. Also the disaggregation part
and the =computing mean_std= part were quite slow.

The vectorization program gave the following speedups:

=make_contexts=: 17_517s -> 440.5s (39.8x thanks to numba)

=compute_disagg=: 7_156s -> 169.2s (42.3x thanks to numpy)

=computing mean_std=: 5_438s -> 17.1 (318x thanks to numpy)

Overall the cumulative time went down from 12 hours to 20 minutes.
Moreover the memory occupation went down by a factor of 4, from
21.3 GB to 5.3 GB.

[[./EDF-disagg.png]]

Since the calculation was dominated by =make_contexts= the game changer
here was the usage of numba. The plot below shows the difference between
having numba and not having numba in engine 3.15:

[[./EDF-numba.png]]

Since =compute_disagg= and =computing mean_std= are not using numba,
there is no difference between the times withing the measuring errors.
Notice that there would be no point in using numba in =compute_disagg=
and =computing mean_std=: they are already well vectorized by numpy and
using numba would not give any significant speedup.

Notice that a calculation involving only fault sources would not see
any speedup since we optimized only point-like sources.
