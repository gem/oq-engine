#+title: The secrets of the datastore, part I (classical)
#+author: Michele Simionato
#+date: April 29th 2021
#+epresent_frame_level: 2

* Introduction
** What I will talk about today  

1. I will present the situation as of current master, engine 3.12
2. I will give you the material, so you do not need to keep notes
3. I will cover how sources, ruptures and logic trees are stored
4. I will cover the high level API to hazardlib (*contexts.py*)
5. Remember the rule: if it is not documented, it is not ready!

** What it is the difference between job, calculation and datastore?

- a job is a database concept ("job 1234 is currently running")
- a calculation is a filesystem concept ("please import the
   calculation /home/michele/oqdata/calc_1234.hdf5").
- a datastore is a Python concept

#+BEGIN_SRC python
from openquake.commonlib import datastore
dstore = datastore.read(-1)  # read the last calculation 
dstore = datastore.read(23)  # open calculation 23 as 'w' or 'r+'
#+END_SRC

** Are there good use cases for calculations not associated to jobs?

Every time you run the tests you get calculations not stored in the
database. For throwaway scripts this also can make sense. But
*it is better to keep jobs and calculations associated*.

A job can be removed from the database without removing its
associated calculation (or viceversa), but it is *not recommended*.

** But why do we have a database?

1. to support *multiple users* (including supporting `--hc`)
2. to store the logs in a way accessible to the WebUI

*datastore.new* and *datastore.read* look at the DB first,
and then at $HOME/oqdata/calc_XXX.hdf5.

This is why *oq dbserver start* is necessary before doing anything,
even in single user installations and even when running the tests.

** Is h5py.File("/home/michele/oqdata/calc_1234.hdf5") a DataStore?

No, it is missing a lot of methods (like *create_df* and *read_df*),
the concept of *parent* DataStore and the *oqparam* dataset.
*Please do not use h5py.File to read datastore files*!

NB: creating a DataStore requires calculation ID & parameters:

#+BEGIN_SRC python
from openquake.commonlib import datastore
dstore = datastore.new(1234, {'calculation_mode': 'classical'})
#+END_SRC

** Is the datastore format stable, finally?

No, however stored pandas *DataFrames* are meant to be stable across
releases. DataFrames are stoed as hdf5 data groups with a dataset
for each column, plus an attribute *__pdcolumns__* with the names
of the columns.

See the change in the *sitecol* between engine 3.11 and 3.12
in case_1.

** Should I use `dframe.to_hdf(path, key)` to store a DataFrame?

Absolutely NOT! *dframe.to_hdf* stores dataframes in a format that
I do not like and has an additional dependency on pytables (which
I do not like either).

#+include: ex1.py src python
#+include: ex2.py src python

* Reading DataFrames

** Example: how to read ruptures as a DataFrame

#+include: ex3.py src python

NB: hazard outputs are not DataFrames (for now)
but they are convertible to DataFrames:

$ oq show hcurves-stats
| site_id | stat | imt     | lvl | value   |
|---------+------+---------+-----+---------|
| 0       | mean | PGA     | 0   | 0.45286 |
| 0       | mean | PGA     | 1   | 0.05589 |
| 0       | mean | PGA     | 2   | 0.00558 |
| 0       | mean | SA(0.1) | 0   | 0.60744 |
| 0       | mean | SA(0.1) | 1   | 0.32547 |
| 0       | mean | SA(0.1) | 2   | 0.19640 |

* Explaining classical ruptures
** What is src_id?

It is the index of the source generating the rupture; the
*source_info* table contains the source name and more.

** What are the fields with underscores, like rrup_?

They are vector-valued fields; for instance `rrup_` contains the
distance rupture-site for all sites

** What are clon_ and clat_?

The longitude and latitude of the closest point to the hazard sites
belonging to the rupture surface.

This is used in disaggregation (by Lon_Lat).

** What is probs_occur_?

The probabilities of occurrence of nonParametric ruptures,
see case_27

** What is grp_id?

It is an index associated to the source group. The way sources
are grouped by the engine is subtle and depends on the tectonic
region type and the source model logic tree, plus model-specific
subtleties (i.e. in the Japan model mutex sources are grouped
together, see case_27).

* Damiano's dream
** The idea

Since the first day I started working on the engine I heard about
the desire of distributing classical calculations by ruptures and
not by sources. The idea was to split the calculation in two:

1. build *all* ruptures and distances from the sources
2. compute the PoES from the stored ruptures and distances

Unfortunaly, it looked *impossible* to keep all that data
(in memory or on disk) for classical calculations.

** Working with contexts

Hazard modelers have to work with ruptures, in formulae like

#+BEGIN_SRC python
mean, stdevs = gsim.get_mean_and_stddevs(
                  sites, rup, dists, imt, stdtypes)
poes = gsim.get_poes(mean, stdevs, imls, trunclevel)
#+END_SRC

Actually rupture-like objects, site-like objects and distance-like
objects (contexts) are enough for hazardlib to work, so storing
such contexts would be extremely useful. This is now possible and
done automatically for few sites (up to *max_sites_disagg=10*).

** RuptureContexts

A *RuptureContext* object contains information about

1. the underlying rupture parameters
2. the site parameters of the affected sites
3. the distances rupture-site for each affected site

A RuptureContext can be passed to every function expecting a
rupture. Rupture parameters and distances are stored as the
*rup* DataFrame.

** ContextMakers

ContextMaker instances keeps inside all the needed information:

- the parameters like *intensity measure types and levels*,
  *truncation level* and *temporal occurrence model*
- the relevant GSIMs as well as the relevant logic tree structure
  stored in a dictionary *gsim -> rlzs*
- a way to retrieve the contexts from the datastore or to compute
  them from the sources:
  see *read_ctxs(dstore)* and *.from_srcs(srcs, sitecol)*

#+include: ex4.py src python

* Postprocessing ruptures

** My recommendation

People wanting to implement new hazard features (conditional
spectrum, aftershocks, ...) should write a *script/notebook*
using the high level API:

- read_cmakers(dstore)
- cmaker.get_pmap(ctxs)
- combine_probs(poes_by_grp, cmakers, rlz_id)

#+include: ex5.py src python

* Logic trees
** How does the engine process the logic tree?

1. read the `job.ini` file
2. read the `gsim_logic_tree` and `source_model_logic_tree` files
3. read the source model files (in parallel, but locally)
4. build a CompositeSourceModel instance that contains all the
   sources ordered in *SourceGroups* by taking into account the
   source model LT
5. store the logic tree structure inside *full_lt*, which contains
   both *source_model_lt* and *gsim_lt*

** What should I (not) do?

Please *do not use* the SourceConverter directly to read
source models. The *right way* is the following:

#+BEGIN_SRC python
>>> from openquake.commonlib import readinput
>>> oqparam = readinput.get_oqparam('job.ini')
>>> csm = readinput.get_composite_source_model(oqparam)
>>> csm.src_groups  # to get the SourceGroups
>>> csm.get_sources()  # to get the sources
#+END_SRC

That takes into account the logic tree automatically.

** Things to know about SourceGroups

1. for each group there is a single TRT
2. for each group there is a single TOM (default PoissonTOM)
3. each group corresponds to a TRT and a list of
   SM realizations denoted by indices *trt_smr*
4. the following relation holds on the indices:
   *trt, smr = divmod(trt_smr, 2**24)*

** Case with a trivial logic tree

Here is how to show the group structure of the CompositeSourceModel:

$ oq show composite_source_model # case_2
| grp_id | trt | smrs | sources |
|--------+-----+------+---------|
| 0      | 0   | 0    | 1       |
| 1      | 1   | 0    | 2       |

** Case with two source models, different sources

$ oq show composite_source_model  # LogicTreeCase1ClassicalPSHA
| grp_id | trt | smrs | sources |
|--------+-----+------+---------|
| 0      | 0   | 0    | 2;0     |
| 1      | 0   | 1    | 2;1     |
| 2      | 1   | 0    | 1;0     |
| 3      | 1   | 1    | 1;1     |

The engine has no trouble managing different sources
with the same source_id (see source_info).

** Same source belonging to multiple source models

The engine has no trouble managing identical sources either:

$ oq show composite_source_model # case_7, "1" in both branches
| grp_id | trt | smrs | sources |
|--------+-----+------+---------|
| 0      | 0   | 0    | 2       |
| 1      | 0   | 0 1  | 1       |

No calculations are wasted, nor disk space

** Simple case with applyToSources

Here source multiplication happens:

$ oq show composite_source_model # case_8
| grp_id | trt | smrs  | sources |
|--------+-----+-------+---------|
| 0      | 0   | 0     | 1;0     |
| 1      | 0   | 0 1 2 | 2       |
| 2      | 0   | 1     | 1;1     |
| 3      | 0   | 2     | 1;2     |

** Nontrivial applyToSources

$ oq show composite_source_model  # case_20
| grp_id | trt | smrs          | sources   |
|--------+-----+---------------+-----------|
| 0      | 0   | 0 1 2 3 4 5   | SFLT1;0   |
| 1      | 0   | 0 1 2 6 7 8   | COMFLT1;0 |
| 2      | 0   | 0 3 6 9       | CHAR1;0   |
| 3      | 0   | 1 4 7 10      | CHAR1;1   |
| 4      | 0   | 2 5 8 11      | CHAR1;2   |
| 5      | 0   | 3 4 5 9 10 11 | COMFLT1;1 |
| 6      | 0   | 6 7 8 9 10 11 | SFLT1;1   |

$ plot_lt classical/case_20/source_model_logic_tree.xml
$ oq show full_lt/source_model_lt|vd

** Case with sampling

$ oq show composite_source_model  # case_16, 10/759_375 samples
| grp_id | trt | smrs    | sources         |
|--------+-----+---------+-----------------|
| 0      | 0   | 0       | 1;0 5;0         |
| 1      | 0   | 0 2     | 4;0             |
| 2      | 0   | 0 3 6 9 | 2;0             |
| 3      | 0   | 0 5     | 3;0             |
| 4      | 0   | 1       | 1;1 3;1 5;1     |
| 5      | 0   | 1 3 7 9 | 4;1             |
| 6      | 0   | 1 7     | 2;1             |
| 7      | 0   | 2       | 1;2 2;2 3;2     |
| 8      | 0   | 2 8     | 5;2             |
| 9      | 0   | 3       | 5;3             |
| 10     | 0   | 3 4     | 1;3             |
| 11     | 0   | 3 7     | 3;3             |
| 12     | 0   | 4       | 2;3 3;4 4;2     |
| 13     | 0   | 4 6     | 5;4             |
| 14     | 0   | 5       | 2;4 4;3         |
| 15     | 0   | 5 6 7   | 1;4             |
| 16     | 0   | 5 7 9   | 5;5             |
| 17     | 0   | 6       | 3;5 4;4         |
| 18     | 0   | 8       | 1;5 2;5 3;6 4;5 |
| 19     | 0   | 9       | 1;6 3;7         |

** Compact representation for the realizations

$ oq show full_lt/source_model_lt|vd
$ oq show realizations
| rlz_id | branch_path   | weight  |
|--------+---------------+---------|
| 0      | 00211222230~0 | 0.10000 |
| 1      | 01031112121~0 | 0.10000 |
| 2      | 01121422231~0 | 0.10000 |
| 3      | 02111002111~0 | 0.10000 |
| 4      | 02120210022~0 | 0.10000 |
| 5      | 02210222020~0 | 0.10000 |
| 6      | 02211404122~0 | 0.10000 |
| 7      | 02231002120~0 | 0.10000 |
| 8      | 03122301131~0 | 0.10000 |
| 9      | 04111202120~0 | 0.10000 |
