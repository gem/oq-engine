# FAQ about running hazard calculations

## classical calculations

### How do I export the hazard curves/maps/uhs for each realization?

By default the engine only exports statistical results, i.e. the mean
hazard curves/maps/uhs. If you want the individual results you must set
`individual_curves=true` in the job.ini files. Please take care: if you have
thousands of realizations (which is quite common) the data transfer
and disk space requirements will be thousands of times larger than
just returning the mean results: the calculation might fail. This is
why by default `individual_curves` is false.

### Argh, I forgot to set `individual_curves`! Must I repeat the calculation?

No, just set `individual_curves=true` in the job.ini and run
```bash
$ oq engine --run job.ini --reuse-hazard --exports csv
```
The individual outputs will be regenerated by reusing the result of the
previous calculation: it will be a lot faster than repeating the calculation
from scratch.

### Argh, I set the wrong `poes` in the job.ini? Must I repeat the calculation?

No, set the right `poes` in the job.ini and as before run
```bash
$ oq engine --run job.ini --reuse-hazard --exports csv
```
Hazard maps and UHS can be regenerated from an existing calculation
quite efficiently.

### How do I set the pointsource_distance?

In several hazard models (for instance Australia, Canada, etc) it is
possible to improve significantly the performance by tuning a parameter
called `pointsource_distance`. The parameter is discussed in the [advanced
manual](https://docs.openquake.org/oq-engine/advanced/common-mistakes.html#pointsource-distance). You can calibrate the value of the `pointsource_distance`
by taking a few reference sites, computing the hazard map values there, and
then playing with the parameter until you get good performance without loosing
too much precision.

## disaggregation calculations

### I am getting an error "disaggregation matrix is too large"!

This means that you have too many disaggregation bins. Please act on the
binning parameters, i.e. on `mag_bin_width`, `distance_bin_width`,
`coordinate_bin_width`, `num_epsilon_bins`. The most relevant parameter is
`coordinate_bin_width` which is quadratic: for instance by changing from
`coordinate_bin_width=0.1` to `coordinate_bin_width=1.0` the size of your
disaggregation matrix will be reduced by 100 times.

## event based calculations

### What is the relation between sources, ruptures, events and realizations?

A single rupture can produce multiple seismic events during the
investigation time. How many depends on the number of stochastic event sets,
on the rupture occurrence rate and on the `ses_seed` parameters, as
[explained here](https://docs.openquake.org/oq-engine/advanced/rupture-sampling.html).
In the engine a rupture is uniquely identified by
a rupture ID, which is a 32 bit positive integer.
Starting from engine 3.7, seismic events are uniquely identified by an
event ID, which is a 32 bit positive integer. The relation
between event ID and rupture ID is given encoded in the `events` table
in the datastore, which also contains the realization associated to the
event. The properties of the
rupture generating the events can be ascertained by looking inside the
`ruptures` table. In particular ther `srcidx` contains the index of the
source that generated the rupture. The `srcidx` can be used to extract
the properties of the sources by looking inside the `source_info` table,
which contains the `source_id` string used in the XML source model.

## general

### Can I run a calculation from a Jupyter notebook?

You can run any kind of calculation from a Jupyter notebook, but usually
calculations are long and it is not convenient to run them interactively.
Scenarios are an exception, since they are usually fast, unless you use
spatial correlation with a lot of sites. Assuming the parameters of the
calculation are in a `job.ini` file you can run the following lines in
the notebook:
```python
In[1]: from openquake.commands.run import run
In[2]: calc = run(['job.ini'])
```
Then you can inspect the contents of the datastore and perform your
postprocessing:
```python
In[3]: calc.datastore.open('r')  # open the datastore for reading
```
The inner format of the datastore is not guaranteed to be the same
across releases and it is not documented, so this approach is
recommended to the most adventurous people.

### how do I plot/analyze/postprocess the results of a calculation?

The official way to plot the result of a calculation is to use the
[QGIS plugin](https://plugins.qgis.org/plugins/svir/). However you
may want a kind of plot which is not available in the plugin, or
you may want to batch-produce hundreds of plots, or you may want to
plot the results of a postprocessing operation. In such cases you
need to use the [extract API](extract-api) and to write your own
plotting/postprocessing code.
