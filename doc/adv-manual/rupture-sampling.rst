Rupture sampling: how does it work?
===================================

In this section we will explain how the sampling of ruptures in event based
calculations works, at least for the case of poissonian sources.
As an example, we will consider the following point source:

>>> from openquake.hazardlib import nrml
>>> src = nrml.get('''\
... <pointSource id="1" name="Point Source"
...              tectonicRegion="Active Shallow Crust">
...     <pointGeometry>
...         <gml:Point><gml:pos>179.5 0</gml:pos></gml:Point>
...         <upperSeismoDepth>0</upperSeismoDepth>
...         <lowerSeismoDepth>10</lowerSeismoDepth>
...     </pointGeometry>
...     <magScaleRel>WC1994</magScaleRel>
...     <ruptAspectRatio>1.5</ruptAspectRatio>
...     <truncGutenbergRichterMFD aValue="3" bValue="1" minMag="5" maxMag="7"/>
...     <nodalPlaneDist>
...         <nodalPlane dip="30" probability="1" strike="45" rake="90" />
...     </nodalPlaneDist>
...     <hypoDepthDist>
...         <hypoDepth depth="4" probability="1"/>
...     </hypoDepthDist>
... </pointSource>''', investigation_time=1, width_of_mfd_bin=1.0)

The source here is particularly simple, with only one
seismogenic depth and one nodal plane. It generates two ruptures,
because with a ``width_of_mfd_bin`` of 1 there are only two magnitudes in
the range from 5 to 7:

>>> [(mag1, rate1), (mag2, rate2)] = src.get_annual_occurrence_rates()
>>> mag1
5.5
>>> mag2
6.5

The occurrence rates are respectively 0.009 and 0.0009. So, if we set
the number of stochastic event sets to 1,000,000

>>> num_ses = 1_000_000

we would expect the first rupture (the one with magnitude 5.5) to
occur around 9000 times and the second rupture (the one with magnitude
6.5) to occur around 900 times. Clearly the exact numbers will depend on
the stochastic seed; if we set

>>> import numpy.random
>>> numpy.random.seed(42)

then we will have

>>> numpy.random.poisson(rate1 * num_ses)
8966
>>> numpy.random.poisson(rate2 * num_ses)
921

These are the number of occurrences of each rupture in the effective
investigation time, i.e. the investigation time multiplied by the
number of stochastic event sets.

The total number of events generated by the source will be

  ``number_of_events = sum(n_occ for each rupture)``

i.e. 8966 + 921 = 9887, with ~91% of the events associated to the first
rupture and ~9% of the events associated to the second rupture.

Since the details of the seed algorithm changes at each release of
the engine, if you run an event based calculation with the same
parameters you will not get exactly the same number of events,
but something close. After running the calculation inside
the datastore, in the ``ruptures`` dataset you will find the two
ruptures, their occurrence rates and their integer number of
occurrences (``n_occ``). If the effective investigation time is large
enough the relation

  ``n_occ ~ occurrence_rate * eff_investigation_time``

will hold. If the effective investigation time is not large enough, or the
occurrence rate is extremely small, then there will be big differences
between the expected number of occurrences and ``n_occ``, as well as a
strong seed dependency.

It is important to notice than in order to determine the effective
investigation time the engine takes into account also the logic tree
and the correct formula to use is:

``eff_investigation_time = investigation_time * num_ses * logic_tree_size``

The ``logic_tree_size`` is equal to the parameter
``number_of_logic_tree_samples`` if it is nonzero; otherwise it is
equal to the number of paths in the full logic tree.

Just to be concrete, if you run a calculation with the same parameters
as described before, but with two GMPEs instead of one (and
``number_of_logic_tree_samples=0``), then the total number
of paths admitted by the logic tree will be 2 and you should expect to get
about twice the number of occurrences for each rupture.

Users wanting to know the nitty-gritty details should look at the
code, inside hazardlib/source/base.py, to the method
``src.sample_ruptures(eff_num_ses, ses_seed)``.
