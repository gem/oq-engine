# Copyright (c) 2010-2013, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Core functionality for the classical PSHA risk calculator.
"""

import random
import collections
import numpy

from django import db

from openquake.hazardlib.geo import mesh
from openquake.risklib import scientific, workflows

from openquake.engine.calculators import post_processing
from openquake.engine.calculators.risk import base, hazard_getters
from openquake.engine.db import models
from openquake.engine.utils import tasks
from openquake.engine import logs, writer
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.calculators.base import signal_task_complete


@tasks.oqtask
@base.count_progress_risk('r')
def event_based(job_id, units, containers, params):
    """
    Celery task for the event based risk calculator.

    :param job_id: the id of the current
        :class:`openquake.engine.db.models.OqJob`
    :param dict units:
      A list of :class:`openquake.risklib.workflows.CalculationUnit` instances
    :param containers:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (e.g. a LossCurve)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
    """

    def profile(name):
        return EnginePerformanceMonitor(
            name, job_id, event_based, tracing=True)

    # Do the job in other functions, such that they can be unit tested
    # without the celery machinery
    event_loss_tables = dict()

    with db.transaction.commit_on_success(using='reslt_writer'):
        for unit in units:
            event_loss_tables[unit.loss_type] = do_event_based(
                unit,
                containers.with_args(loss_type=unit.loss_type),
                params, profile)
    num_items = base.get_num_items(units)
    signal_task_complete(job_id=job_id,
                         num_items=num_items,
                         event_loss_tables=event_loss_tables)
event_based.ignore_result = False


def do_event_based(unit, containers, params, profile):
    """
    See `event_based` for a description of the params

    :returns: the event loss table generated by `units`
    """
    for hazard_output_id, outputs in unit.workflow(
            unit.loss_type,
            unit.getter(profile('getting data')),
            profile('computing individual risk')):

        if not unit.workflow.assets:
            logs.LOG.info("Exit from task as no asset could be processed")
            return collections.Counter()

        if params.sites_disagg:
            with profile('disaggregating results'):
                rupture_ids = unit.workflow.event_loss_table.keys()
                disagg_outputs = disaggregate(outputs, rupture_ids, params)
        else:
            disagg_outputs = None

        with profile('saving individual risk'):
            save_individual_outputs(
                containers.with_args(hazard_output_id=hazard_output_id),
                outputs, disagg_outputs, params)

    with profile('computing risk statistics'):
        stats = unit.workflow.statistics(
            unit.getter.weights(), params.quantiles, post_processing)

    with profile('saving risk statistics'):
        if stats is not None:
            save_statistical_output(
                containers.with_args(hazard_output_id=None), stats, params)

    return unit.workflow.event_loss_table


def save_individual_outputs(containers, outputs, disagg_outputs, params):
    containers.write(
        outputs.assets, outputs.loss_curves, output_type="loss_curve")

    containers.write_all(
        "poe", params.conditional_loss_poes,
        outputs.loss_maps,
        outputs.assets,
        output_type="loss_map")

    if disagg_outputs is not None:
        containers.write(
            disagg_outputs.assets_disagg,
            disagg_outputs.magnitude_distance,
            disagg_outputs.fractions,
            output_type="loss_fraction",
            variable="magnitude_distance")
        containers.write(
            disagg_outputs.assets_disagg,
            disagg_outputs.coordinate, disagg_outputs.fractions,
            output_type="loss_fraction",
            variable="coordinate")

    if outputs.insured_curves is not None:
        containers.write(
            outputs.assets, outputs.insured_curves,
            output_type="loss_curve", insured=True)


def save_statistical_output(containers, stats, params):
    containers.write(
        stats.assets, stats.mean_curves,
        output_type="loss_curve", statistics="mean")

    containers.write_all(
        "poe", params.conditional_loss_poes, stats.mean_maps,
        stats.assets, output_type="loss_map", statistics="mean")

    # quantile curves and maps
    containers.write_all(
        "quantile", params.quantiles, stats.quantile_curves,
        stats.assets, output_type="loss_curve", statistics="quantile")

    if params.quantiles:
        for quantile, maps in zip(params.quantiles, stats.quantile_maps):
            containers.write_all(
                "poe", params.conditional_loss_poes, maps,
                stats.assets, output_type="loss_map",
                statistics="quantile", quantile=quantile)


class DisaggregationOutputs(object):
    def __init__(self, assets_disagg, magnitude_distance,
                 coordinate, fractions):
        self.assets_disagg = assets_disagg
        self.magnitude_distance = magnitude_distance
        self.coordinate = coordinate
        self.fractions = fractions


def disaggregate(outputs, rupture_ids, params):
    """
    Compute disaggregation outputs given the individual `outputs` and `params`

    :param outputs:
      an instance of
      :class:`openquake.risklib.workflows.ProbabilisticEventBased.Output`
    :param params:
      an instance of :class:`..base.CalcParams`
    :param list rupture_ids:
      a list of :class:`openquake.engine.db.models.SESRupture` IDs
    :returns:
      an instance of :class:`DisaggregationOutputs`
    """
    def disaggregate_site(site, loss_ratios):
        for fraction, rupture_id in zip(loss_ratios, rupture_ids):

            rupture = models.SESRupture.objects.get(pk=rupture_id)
            s = rupture.surface
            m = mesh.Mesh(numpy.array([site.x]), numpy.array([site.y]), None)

            mag = numpy.floor(rupture.magnitude / params.mag_bin_width)
            dist = numpy.floor(
                s.get_joyner_boore_distance(m))[0] / params.distance_bin_width

            closest_point = iter(s.get_closest_points(m)).next()
            lon = closest_point.longitude / params.coordinate_bin_width
            lat = closest_point.latitude / params.coordinate_bin_width

            yield "%d,%d" % (mag, dist), "%d,%d" % (lon, lat), fraction

    assets_disagg = []
    disagg_matrix = []

    for asset, losses in zip(outputs.assets, outputs.loss_matrix):
        if asset.site in params.sites_disagg:
            disagg_matrix.extend(list(disaggregate_site(asset.site, losses)))
            assets_disagg.append(asset)
    if assets_disagg:
        magnitudes, coordinates, fractions = zip(*disagg_matrix)
    else:
        magnitudes, coordinates, fractions = [], [], []

    return DisaggregationOutputs(
        assets_disagg, magnitudes, coordinates, fractions)


class EventBasedRiskCalculator(base.RiskCalculator):
    """
    Probabilistic Event Based PSHA risk calculator. Computes loss
    curves, loss maps, aggregate losses and insured losses for a given
    set of assets.
    """

    #: The core calculation celery task function
    core_calc_task = event_based

    def __init__(self, job):
        super(EventBasedRiskCalculator, self).__init__(job)
        self.event_loss_tables = dict()
        self.rnd = random.Random()
        self.rnd.seed(self.rc.master_seed)

        # seed the rng to generate different seeds per-each output
        # (i.e. each hazard realization). This allows different tasks
        # to generate the same random numbers given an output. These
        # seeds will be used when computing ground motion values on
        # the fly in order to provide the right correlation between
        # random numbers generated across tasks

        rnd = random.Random()
        rnd.seed(self.rc.master_seed)
        self.hazard_seeds = [rnd.randint(0, models.MAX_SINT_32)
                             for _ in self.rc.hazard_outputs()]

    def task_completed_hook(self, message):
        """
        Updates the event loss table
        """
        for loss_type in base.loss_types(self.risk_models):
            task_loss_table = message['event_loss_tables'][loss_type]
            self.event_loss_tables[loss_type] += task_loss_table

    def validate_hazard(self):
        """
        Check that the given hazard comes from an event based calculation
        """
        super(EventBasedRiskCalculator, self).validate_hazard()
        if self.rc.hazard_calculation:
            if self.rc.hazard_calculation.calculation_mode != "event_based":
                raise RuntimeError(
                    "The provided hazard calculation ID "
                    "is not an event based calculation")
        elif not self.rc.hazard_output.output_type in ["gmf", "ses"]:
            raise RuntimeError(
                "The provided hazard output is not a gmf collection")

    def get_taxonomies(self):
        """
        If insured losses are required we check for the presence of
        the deductible and insurance limit
        """
        taxonomies = super(EventBasedRiskCalculator, self).get_taxonomies()

        assets_without_limits = (
            self.rc.exposure_model.exposuredata_set.filter(
                (db.models.Q(cost__deductible_absolute__isnull=True) |
                 db.models.Q(cost__insurance_limit_absolute__isnull=True))))

        if self.rc.insured_losses and assets_without_limits.exists():
            raise RuntimeError(
                "Deductible or insured limit missing in exposure for "
                "some assets")

        # FIXME(lp). Validate sites_disagg to ensure non-empty outputs

        return taxonomies

    def post_process(self):
        """
          Compute aggregate loss curves and event loss tables
        """
        with EnginePerformanceMonitor('post processing', self.job.id):

            time_span, tses = self.hazard_times()
            for loss_type, event_loss_table in self.event_loss_tables.items():
                for hazard_output in self.rc.hazard_outputs():

                    event_loss = models.EventLoss.objects.create(
                        output=models.Output.objects.create_output(
                            self.job,
                            "Event Loss Table. type=%s, hazard=%s" % (
                                loss_type, hazard_output.id),
                            "event_loss"),
                        loss_type=loss_type,
                        hazard_output=hazard_output)
                    inserter = writer.CacheInserter(models.EventLossData, 9999)

                    ruptures = models.SESRupture.objects.filter(
                        ses__ses_collection__lt_realization=
                        hazard_output.output_container.lt_realization)

                    for rupture in ruptures:
                        if rupture.id in event_loss_table:
                            inserter.add(
                                models.EventLossData(
                                    event_loss_id=event_loss.id,
                                    rupture_id=rupture.id,
                                    aggregate_loss=event_loss_table[
                                        rupture.id]))
                    inserter.flush()

                    aggregate_losses = [
                        event_loss_table[rupture.id]
                        for rupture in ruptures
                        if rupture.id in event_loss_table]

                    if aggregate_losses:
                        aggregate_loss_losses, aggregate_loss_poes = (
                            scientific.event_based(
                                aggregate_losses, tses=tses,
                                time_span=time_span,
                                curve_resolution=self.rc.loss_curve_resolution
                            ))

                        models.AggregateLossCurveData.objects.create(
                            loss_curve=models.LossCurve.objects.create(
                                aggregate=True, insured=False,
                                hazard_output=hazard_output,
                                loss_type=loss_type,
                                output=models.Output.objects.create_output(
                                    self.job,
                                    "aggregate loss curves. "
                                    "loss_type=%s hazard=%s" % (
                                        loss_type, hazard_output),
                                    "agg_loss_curve")),
                            losses=aggregate_loss_losses,
                            poes=aggregate_loss_poes,
                            average_loss=scientific.average_loss(
                                aggregate_loss_losses, aggregate_loss_poes))

    def calculation_unit(self, loss_type, assets):
        """
        :returns:
          a list of instances of `..base.CalculationUnit` for the given
          `assets` to be run in the celery task
        """

        # assume all assets have the same taxonomy
        taxonomy = assets[0].taxonomy
        risk_model = self.risk_models[taxonomy][loss_type]

        time_span, tses = self.hazard_times()

        return workflows.CalculationUnit(
            loss_type,
            workflows.ProbabilisticEventBased(
                risk_model.vulnerability_function,
                self.rnd.randint(0, models.MAX_SINT_32),
                self.rc.asset_correlation,
                time_span, tses,
                self.rc.loss_curve_resolution,
                self.rc.conditional_loss_poes,
                self.rc.insured_losses),
            hazard_getters.GroundMotionValuesGetter(
                self.rc.hazard_outputs(),
                assets,
                self.rc.best_maximum_distance,
                risk_model.imt,
                self.hazard_seeds))

    def hazard_times(self):
        """
        Return the hazard investigation time related to the ground
        motion field and the so-called time representative of the
        stochastic event set
        """
        time_span = self.hc.investigation_time
        return time_span, self.hc.ses_per_logic_tree_path * time_span

    @property
    def calculator_parameters(self):
        """
        Calculator specific parameters
        """

        return base.make_calc_params(
            conditional_loss_poes=self.rc.conditional_loss_poes or [],
            quantiles=self.rc.quantile_loss_curves or [],
            insured_losses=self.rc.insured_losses,
            sites_disagg=self.rc.sites_disagg or [],
            mag_bin_width=self.rc.mag_bin_width,
            distance_bin_width=self.rc.distance_bin_width,
            coordinate_bin_width=self.rc.coordinate_bin_width)

    def create_outputs(self, hazard_output):
        """
        Add Insured Curve output containers
        """
        # includes loss curves and loss maps
        outputs = super(EventBasedRiskCalculator, self).create_outputs(
            hazard_output)

        for loss_type in base.loss_types(self.risk_models):
            if loss_type != "fatalities":
                if self.rc.insured_losses:
                    name = "insured loss curves. type=%s hazard %s" % (
                        loss_type, hazard_output),
                    outputs.set(
                        models.LossCurve.objects.create(
                            insured=True,
                            loss_type=loss_type,
                            hazard_output=hazard_output,
                            output=models.Output.objects.create_output(
                                self.job, name, "loss_curve")))

            if self.rc.sites_disagg:
                name = ("loss fractions. type=%s variable=magnitude_distance "
                        "hazard=%s" % (loss_type, hazard_output))
                outputs.set(
                    models.LossFraction.objects.create(
                        output=models.Output.objects.create_output(
                            self.job, name, "loss_fraction"),
                        hazard_output=hazard_output,
                        loss_type=loss_type,
                        variable="magnitude_distance"))
                name = ("loss fractions. type=%s variable=coordinates "
                        "hazard=%s" % (loss_type, hazard_output))
                outputs.set(models.LossFraction.objects.create(
                    output=models.Output.objects.create_output(
                        self.job, name, "loss_fraction"),
                    hazard_output=hazard_output,
                    loss_type=loss_type,
                    variable="coordinate"))

        return outputs

    def create_statistical_outputs(self):
        for loss_type in base.loss_types(self.risk_models):
            self.event_loss_tables[loss_type] = collections.Counter()
        return super(
            EventBasedRiskCalculator, self).create_statistical_outputs()
