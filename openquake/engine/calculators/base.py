# Copyright (c) 2010-2013, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""Base code for calculator classes."""

import math

import kombu

import openquake.engine

from openquake.engine import logs
from openquake.engine.db import models
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import config, tasks, general

# Routing key format string for communication between tasks and the control
# node.
ROUTING_KEY_FMT = 'oq.job.%(job_id)s.tasks'


class Calculator(object):
    """
    Base class for all calculators.

    :param job: :class:`openquake.engine.db.models.OqJob` instance.
    """

    #: The core calculation Celery task function, which accepts the arguments
    #: generated by :func:`task_arg_gen`.
    core_calc_task = None

    def __init__(self, job):
        self.job = job
        self.num_tasks = None
        self.progress = dict(total=0, computed=0, in_queue=0)

    def monitor(self, operation):
        """
        Return an EnginePerformanceMonitor to be used in the calculator,
        for code running on the master node.
        """
        return EnginePerformanceMonitor(
            operation, self.job.id, profile_pymem=True, profile_pgmem=True,
            tracing=True, flush=True)

    def task_arg_gen(self, block_size):
        """
        Generator function for creating the arguments for each task.

        Subclasses must implement this.

        :param int block_size:
            The number of work items per task (sources, sites, etc.).
        """
        raise NotImplementedError

    def block_size(self):
        """
        Number of work items per task.

        Subclasses must implement this.
        """
        raise NotImplementedError()

    def concurrent_tasks(self):
        """
        Number of tasks to be in queue at any given time.

        Subclasses must implement this.
        """
        raise NotImplementedError()

    def calc_num_tasks(self):
        """
        Number of tasks to spawn.
        Subclasses must implement this.
        """

    # NB: there is an issue here, because a single calculation can run
    # two bunches of parallel tasks: one in the execute phase and one
    # in the post-processing phase; however the job_stats table has
    # room only for a single num_tasks column;
    # see https://bugs.launchpad.net/oq-engine/+bug/1239529
    def record_init_stats(self, num_tasks=None):
        """
        Record some basic job stats, including the number of sites,
        realizations (end branches), and total number of tasks for the job.

        This should be run between the `pre-execute` and `execute` phases, once
        the job has been fully initialized.
        """
        # Record num sites, num realizations, and num tasks.
        num_sites = len(self.hc.points_to_compute())
        realizations = models.LtRealization.objects.filter(
            hazard_calculation=self.hc.id)
        num_rlzs = realizations.count()

        [job_stats] = models.JobStats.objects.filter(oq_job=self.job.id)
        job_stats.num_sites = num_sites
        job_stats.num_tasks = num_tasks or self.calc_num_tasks()
        job_stats.num_realizations = num_rlzs
        job_stats.save()

    def parallelize(self, task_func, task_arg_gen):
        """
        Given a callable and a task arg generator, apply the callable to
        the arguments in parallel. For efficiency the tasks are spawned in
        chunks. Here is how it works.

        Suppose you are running a computation with 100,000 sources and
        10 realizations: then 1,000,000 arguments are generated (if
        the calculation is an event based one this number must be
        multiplied by the number of stochastic event sets). Generating
        a million tasks would be foolish and inefficient, since most
        of the time would be spent in passing arguments via
        rabbitmq. This method implements a chunking mechanism to
        collect the arguments and generate a total number of tasks
        which is always lower than the openquake.cfg parameter
        `concurrent_tasks`. In the cluster the number of
        concurrent_tasks is set to 512. If there are 1,000,000
        arguments the algorithm divides num_args / maxtasks and finds
        out a chunksize of 1954, by rounding to the closest upper
        integer. That means that we will generated 1,000,000 / 1954 =
        512 tasks (the division is rounded to the closest upper
        integer) each with 1954 arguments except the last one which
        will have 1506 arguments.  Each task will call the task_func with
        its arguments.

        Every time a task completes the method .log_percent() is called
        and a progress message is displayed if the percentage has changed.

        :param task_func: a `celery` task callable
        :param task_args: an iterable over positional arguments

        NB: if the environment variable OQ_NO_DISTRIBUTE is set the
        tasks are run sequentially in the current process.
        """
        self.taskname = task_func.__name__
        maxtasks = self.concurrent_tasks()
        arglist = list(task_arg_gen)
        chunksize = int(math.ceil(float(len(arglist)) / maxtasks))
        chunks = list(general.block_splitter(arglist, chunksize))
        self.num_tasks = len(chunks)
        self.tasksdone = 0
        self.percent = 0.0
        logs.LOG.progress(
            'spawning %d tasks of kind %s, chunksize=%d',
            self.num_tasks, self.taskname, chunksize)
        tasks.parallelize(task_func, chunks, self.log_percent)

    def log_percent(self, dummy):
        """Log the percentage of tasks completed"""
        self.tasksdone += 1
        percent = math.ceil(float(self.tasksdone) / self.num_tasks * 100)
        if percent > self.percent:
            logs.LOG.progress('> %s %3d%% complete', self.taskname, percent)
            self.percent = percent

    def get_task_complete_callback(self, task_arg_gen, block_size,
                                   concurrent_tasks):
        """
        Create the callback which responds to a task completion signal. In some
        cases, the response is simply to enqueue the next task (if there is
        any work left to be done).

        :param task_arg_gen:
            The task arg generator, so the callback can get the next set of
            args and enqueue the next task.
        :param int block_size:
            The (maximum) number of work items to pass to a given task.
        :param int concurrent_tasks:
            The (maximum) number of tasks that should be in queue at any time.
        :return:
            A callback function which responds to a task completion signal.
            A response typically includes enqueuing the next task and updating
            progress counters.
        """

        def callback(body, message):
            """
            :param dict body:
                ``body`` is the message sent by the task. The dict should
                contain 2 keys: `job_id` and `num_items` (to indicate the
                number of sources computed).

                Both values are `int`.
            :param message:
                A :class:`kombu.transport.pyamqplib.Message`, which contains
                metadata about the message (including content type, channel,
                etc.). See kombu docs for more details.
            """
            job_id = body['job_id']
            num_items = body['num_items']

            assert job_id == self.job.id
            self.progress['computed'] += num_items

            self.task_completed_hook(body)

            logs.log_percent_complete(job_id, "hazard")
            logs.log_percent_complete(job_id, "risk")

            # Once we receive a completion signal, enqueue the next
            # piece of work (if there's anything left to be done).
            try:
                queue_next(self.core_calc_task, task_arg_gen.next())
            except StopIteration:
                # There are no more tasks to dispatch; now we just need
                # to wait until all tasks signal completion.
                self.progress['in_queue'] -= 1

            message.ack()
            logs.LOG.info('A task was completed. Tasks now in queue: %s'
                          % self.progress['in_queue'])

        return callback

    def task_completed_hook(self, body):
        """
        Performs an action when a task is completed successfully.
        :param dict body: the message sent by the task. It contains at least
        the keys `job_id` and `num_items`. They idea is to add additional
        keys and then process them in the hook. Notice that the message
        is sent by using
        `openquake.engine.calculators.base.signal_task_complete`.
        """
        pass

    def pre_execute(self):
        """
        Override this method in subclasses to record pre-execution stats,
        initialize result records, perform detailed parsing of input data, etc.
        """

    def execute(self):
        """
        Calculation work is parallelized over sources, which means that each
        task will compute hazard for all sites but only with a subset of the
        seismic sources defined in the input model.

        The general workflow is as follows:

        1. Fill the queue with an initial set of tasks. The number of initial
        tasks is configurable using the `concurrent_tasks` parameter in the
        `[hazard]` section of the OpenQuake config file.

        2. Wait for tasks to signal completion (via AMQP message) and enqueue a
        new task each time another completes. Once all of the job work is
        enqueued, we just wait until all of the tasks conclude.

        It is possible to override this method to change the distribution
        mechanism.
        """
        if openquake.engine.no_distribute():
            logs.LOG.warn('Calculation task distribution is disabled')
        # The following two counters are in a dict so that we can use them in
        # the closures below.
        # When `self.progress['compute']` becomes equal to
        # `self.progress['total']`, `execute` can conclude.

        task_gen = self.task_arg_gen(self.block_size())
        exchange, conn_args = exchange_and_conn_args()

        routing_key = ROUTING_KEY_FMT % dict(job_id=self.job.id)
        task_signal_queue = kombu.Queue(
            'tasks.job.%s' % self.job.id, exchange=exchange,
            routing_key=routing_key, durable=False, auto_delete=True)

        with kombu.BrokerConnection(**conn_args) as conn:
            task_signal_queue(conn.channel()).declare()
            with conn.Consumer(
                task_signal_queue,
                callbacks=[self.get_task_complete_callback(
                    task_gen, self.block_size(), self.concurrent_tasks())]):

                # First: Queue up the initial tasks.
                for _ in xrange(self.concurrent_tasks()):
                    try:
                        queue_next(self.core_calc_task, task_gen.next())
                    except StopIteration:
                        # If we get a `StopIteration` here, that means we have
                        # a number of tasks < concurrent_tasks.
                        # This basically just means that we could be
                        # under-utilizing worker node resources.
                        break
                    else:
                        self.progress['in_queue'] += 1

                logs.LOG.info('Tasks now in queue: %s'
                              % self.progress['in_queue'])

                while (self.progress['computed'] < self.progress['total']):
                    # This blocks until a message is received.
                    # Once we receive a completion signal, enqueue the next
                    # piece of work (if there's anything left to be done).
                    # (The `task_complete_callback` will handle additional
                    # queuing.)
                    conn.drain_events()
        logs.LOG.progress("calculation 100% complete")

    def post_execute(self):
        """
        Override this method in subclasses to any necessary post-execution
        actions, such as the consolidation of partial results.
        """

    def post_process(self):
        """
        Override this method in subclasses to perform post processing steps,
        such as computing mean results from a set of curves or plotting maps.
        """

    def _get_outputs_for_export(self):
        """
        Util function for getting :class:`openquake.engine.db.models.Output`
        objects to be exported.
        """
        raise NotImplementedError()

    def _do_export(self, output_id, export_dir, export_type):
        """
        Perform a single export.
        """
        raise NotImplementedError()

    def export(self, *args, **kwargs):
        """
        If requested by the user, automatically export all result artifacts to
        the specified format. (NOTE: The only export format supported at the
        moment is NRML XML.

        :param exports:
            Keyword arg. List of export types.
        :returns:
            A list of the export filenames, including the absolute path to each
            file.
        """
        exported_files = []

        with logs.tracing('exports'):
            if 'exports' in kwargs:
                outputs = self._get_outputs_for_export()

                for export_type in kwargs['exports']:
                    for output in outputs:
                        with self.monitor('exporting %s to %s'
                                          % (output.output_type, export_type)):
                            fname = self._do_export(
                                output.id,
                                self.job.calculation.export_dir,
                                export_type
                            )
                            exported_files.append(fname)

        return exported_files

    def clean_up(self, *args, **kwargs):
        """Implement this method in subclasses to perform clean-up actions
           like garbage collection, etc."""


def exchange_and_conn_args():
    """
    Helper method to setup an exchange for task communication and the args
    needed to create a broker connection.
    """

    amqp_cfg = config.get_section('amqp')
    exchange = kombu.Exchange(amqp_cfg['task_exchange'], type='direct')

    conn_args = {
        'hostname': amqp_cfg['host'],
        'userid': amqp_cfg['user'],
        'password': amqp_cfg['password'],
        'virtual_host': amqp_cfg['vhost'],
    }

    return exchange, conn_args


def queue_next(task_func, task_args):
    """
    :param task_func:
        A Celery task function, to be enqueued with the next set of args in
        ``task_arg_gen``.
    :param task_args:
        A set of arguments which match the specified ``task_func``.

    .. note::
        This utility function was added to make for easier mocking and testing
        of the "plumbing" which handles task queuing (such as the various "task
        complete" callback functions).
    """
    if openquake.engine.no_distribute():
        task_func(*task_args)
    else:
        task_func.apply_async(task_args)


def signal_task_complete(**kwargs):
    """
    Send a signal back through a dedicated queue to the 'control node' to
    notify of task completion and the number of items processed.

    Signalling back this metric is needed to tell the control node when it can
    conclude its `execute` phase.

    :param kwargs:
        Arbitrary message parameters. Anything in this dict will go into the
        "task complete" message.

        Typical message parameters would include `job_id` and `num_items` (to
        indicate the number of work items that the task has processed).

        .. note::
            `job_id` is required for routing the message. All other parameters
            can be treated as optional.
    """
    msg = kwargs
    # here we make the assumption that the job_id is in the message kwargs
    job_id = kwargs['job_id']

    exchange, conn_args = exchange_and_conn_args()

    routing_key = ROUTING_KEY_FMT % dict(job_id=job_id)

    with kombu.BrokerConnection(**conn_args) as conn, conn.Producer(
            exchange=exchange, routing_key=routing_key,
            serializer='pickle') as producer:
        producer.publish(msg)
