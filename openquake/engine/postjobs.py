# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
# 
# Copyright (C) 2025, GEM Foundation
# 
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
# 
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Postprocessing operations collecting together the results of Global Risk Model
calculations.
"""

import os
import logging
import tempfile
from openquake.baselib import hdf5, config, performance
from openquake.commonlib import datastore
from openquake.calculators import base, export


def build_ses(dstore, jobs, out_file):
    """
    Build an HDF5 file with the global SES by importing the ruptures
    generated by each job.
    """
    with performance.Monitor(measuremem=True, h5=dstore) as mon:
        fnames = [datastore.read(job.calc_id).filename for job in jobs]
        logging.warning(f'Saving {out_file}')
        with hdf5.File(out_file, 'w') as h5:
            base.import_sites_hdf5(h5, fnames)
            base.import_ruptures_hdf5(h5, fnames)
            h5['oqparam'] = jobs[0].get_oqparam()
    print(mon)


def _export_import(job, output_type, dstore):
    # export the CSV files associated to the output type from the
    # calculation and import them in the workflow datastore
    with datastore.read(job.calc_id) as job_ds:
        oq = job_ds['oqparam']
        aggby = set()
        for agg in oq.aggregate_by:
            aggby.update(agg)
        str_fields = ['loss_type', 'taxonomy'] + sorted(aggby)
        job_ds.export_dir = config.directory.custom_tmp or tempfile.gettempdir()
        for fname in export.export((output_type, 'csv'), job_ds):
            table = os.path.basename(fname).rsplit('_', 1)[0]
            # i.e. /tmp/aggexp_tags-NAME_1_27436.csv => aggexp_tags-NAME_1
            dstore.import_csv(fname, table, str_fields, {'job': job.name})
            os.remove(fname)  # remove only if the import succeeded


# tested in test_workflow
def import_risk(dstore, jobs, out_types='aggexp_tags aggrisk avg_losses_by aggcurves'.
                split()):
    """
    Import the output types 'aggexp_tags', 'aggrisk', 'avg_losses_by', 'aggcurves'
    from all the given jobs inside the workflow datastore.
    """
    with performance.Monitor(measuremem=True, h5=dstore) as mon:
        for job in jobs:
            with job:
                for out_type in out_types:
                    _export_import(job, out_type, dstore)
    print(mon)
