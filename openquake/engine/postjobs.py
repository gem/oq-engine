# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
# 
# Copyright (C) 2025, GEM Foundation
# 
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
# 
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Postprocessing operations collecting together the results of Global Risk Model
calculations.
"""

import os
import ast
import logging
import tempfile
from openquake.baselib import hdf5, config, performance
from openquake.hazardlib import return_periods
from openquake.commonlib import datastore
from openquake.calculators import base, export, views


def build_ses(dstore, calcs, out_file):
    """
    Build an HDF5 file with the global SES by importing the ruptures
    generated by each job.
    """
    with performance.Monitor(measuremem=True, h5=dstore) as mon:
        dstores = [datastore.read(calc) for calc in calcs]
        fnames = [ds.filename for ds in dstores]
        logging.warning(f'Saving {out_file}')
        with hdf5.File(out_file, 'w') as h5:
            logging.info('Importing sites')
            base.import_sites_hdf5(h5, fnames)
            logging.info('Importing ruptures')
            base.import_ruptures_hdf5(h5, fnames)
            h5['oqparam'] = dstores[0]['oqparam']
    print(mon)


def _export_import(name, calc_id, output_type, dstore):
    # export the CSV files associated to the output type from the
    # calculation and import them in the workflow datastore
    with datastore.read(calc_id) as calc_ds:
        oq = calc_ds['oqparam']
        aggby = set()
        for agg in oq.aggregate_by:
            aggby.update(agg)
        str_fields = ['loss_type', 'taxonomy', 'MACRO_TAXONOMY'] + sorted(aggby)
        calc_ds.export_dir = (config.directory.custom_tmp or
                              tempfile.gettempdir())
        if output_type == 'hmaps':
            rps = return_periods(oq.investigation_time, oq.poes)
            breakpoint()
            renamedict = {}
        else:
            renamedict = {}
        for fname in export.export((output_type, 'csv'), calc_ds):
            table = os.path.basename(fname).rsplit('_', 1)[0]
            # i.e. /tmp/aggexp_tags-NAME_1_27436.csv => aggexp_tags-NAME_1
            logging.info(f'Importing {table} for {name} [{calc_id}]')
            dstore.import_csv(fname, table, str_fields, renamedict, {'calc': name})
            os.remove(fname)  # remove only if the import succeeded


# tested in test_workflow
def import_outputs(dstore, calcs, out_types):
    """
    Import the given output types from all the jobs inside the workflow
    """
    wf = dstore.read_df('workflow', 'calc_id')
    with performance.Monitor(measuremem=True, h5=dstore) as mon:
        for calc_id in calcs:
            name = wf.loc[calc_id]['name']
            for out_type in out_types:
                _export_import(name, calc_id, out_type, dstore)
    print(mon)


def post_aelo(dstore, calcs):
    """
    Executed by the AELO tests; save 'asce/XXX.org' files
    """
    assert os.path.exists('asce'), 'You are not in the mosaic directory!'
    asce = {}
    for calc in calcs:
        dstore = datastore.read(calc)
        model = dstore['oqparam'].mosaic_model
        asce[model + '07'] = views.view('asce:07', dstore)
        asce[model + '41'] = views.view('asce:41', dstore)
    for name, table in asce.items():
        fname = os.path.abspath(f'asce/{name}.org')
        with open(fname, 'w') as f:
            print(views.text_table(table[1:], table[0], ext='org'), file=f)
        print(f'Stored {fname}')


def main(postjob: str, workflow_id: int, calc_id: int, arg: str):
    """
    Useful for debugging errors in postjobs
    """
    postjob = globals()[postjob]
    with datastore.read(workflow_id, 'r+') as dstore:
        args = [ast.literal_eval(a) for a in arg]
        postjob(dstore, [calc_id], *args)
main.postjob = dict(help="name of the postjob operation",
                    choices=['build_ses', 'import_outputs', 'post_aelo'])
main.workflow_id = "ID of the workflow calculation"
main.calc_id = "ID of the job to apply the postjob operation"
main.arg = dict(help="Extra arguments (literals)", nargs='*')
