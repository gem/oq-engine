# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
# 
# Copyright (C) 2025, GEM Foundation
# 
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
# 
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Postprocessing operations collecting together the results of Global Risk Model
calculations.
"""

import os
import logging
import tempfile
from openquake.baselib import hdf5, config, performance
from openquake.commonlib import datastore
from openquake.calculators import base, export


def build_ses(dstore, calcs, out_file):
    """
    Build an HDF5 file with the global SES by importing the ruptures
    generated by each job.
    """
    with performance.Monitor(measuremem=True, h5=dstore) as mon:
        dstores = [datastore.read(calc) for calc in calcs]
        fnames = [ds.filename for ds in dstores]
        logging.warning(f'Saving {out_file}')
        with hdf5.File(out_file, 'w') as h5:
            logging.info('Importing sites')
            base.import_sites_hdf5(h5, fnames)
            logging.info('Importing ruptures')
            base.import_ruptures_hdf5(h5, fnames)
            h5['oqparam'] = dstores[0]['oqparam']
    print(mon)


def _export_import(name, calc_id, output_type, dstore):
    # export the CSV files associated to the output type from the
    # calculation and import them in the workflow datastore
    with datastore.read(calc_id) as calc_ds:
        oq = calc_ds['oqparam']
        aggby = set()
        for agg in oq.aggregate_by:
            aggby.update(agg)
        str_fields = ['loss_type', 'taxonomy', 'MACRO_TAXONOMY'] + sorted(aggby)
        calc_ds.export_dir = (config.directory.custom_tmp or
                              tempfile.gettempdir())
        for fname in export.export((output_type, 'csv'), calc_ds):
            table = os.path.basename(fname).rsplit('_', 1)[0]
            # i.e. /tmp/aggexp_tags-NAME_1_27436.csv => aggexp_tags-NAME_1
            logging.info(f'Importing {table} for {name} [{calc_id}]')
            dstore.import_csv(fname, table, str_fields, {'calc': name})
            os.remove(fname)  # remove only if the import succeeded


# tested in test_workflow
def import_outputs(dstore, calcs, out_types):
    """
    Import the given output types from all the jobs inside the workflow
    """
    wf = dstore.read_df('workflow', 'calc_id')
    with performance.Monitor(measuremem=True, h5=dstore) as mon:
        for calc_id in calcs:
            name = wf.loc[calc_id]['name']
            for out_type in out_types:
                _export_import(name, calc_id, out_type, dstore)
    print(mon)
